\documentclass[a4paper,14pt]{article}

%%% Работа с русским языком
\usepackage{cmap}					% поиск в PDF
\usepackage{mathtext} 				% русские буквы в формулах
\usepackage[T2A]{fontenc}			% кодировка
\usepackage[utf8]{inputenc}			% кодировка исходного текста
\usepackage[english,russian]{babel}	% локализация и переносы
\usepackage{indentfirst}
\frenchspacing

\newcommand{\bz}{\mathbf{z}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bp}{\mathbf{p}}
\newcommand{\bq}{\mathbf{q}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bZ}{\mathbf{Z}}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\bH}{\mathbf{H}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\bP}{\mathbf{P}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bC}{\mathbf{C}}
\newcommand{\bE}{\mathbf{E}}
\newcommand{\bF}{\mathbf{F}}
\newcommand{\bomega}{\boldsymbol{\omega}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bgamma}{\boldsymbol{\gamma}}
\newcommand{\bdelta}{\boldsymbol{\delta}}
\newcommand{\bPsi}{\boldsymbol{\Psi}}
\newcommand{\bpsi}{\boldsymbol{\psi}}
\newcommand{\bxi}{\boldsymbol{\xi}}
\newcommand{\bchi}{\boldsymbol{\chi}}
\newcommand{\bzeta}{\boldsymbol{\zeta}}
\newcommand{\blambda}{\boldsymbol{\lambda}}
\newcommand{\beps}{\boldsymbol{\varepsilon}}
\newcommand{\bZeta}{\boldsymbol{Z}}
% mathcal
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cW}{\mathcal{W}}

\newcommand{\dH}{\mathbb{H}}
\newcommand{\dR}{\mathbb{R}}
\newcommand{\dE}{\mathbb{E}}
% transpose
\newcommand{\T}{^{\mathsf{T}}}

\renewcommand{\epsilon}{\ensuremath{\varepsilon}}
\renewcommand{\phi}{\ensuremath{\varphi}}
\renewcommand{\kappa}{\ensuremath{\varkappa}}
\renewcommand{\le}{\ensuremath{\leqslant}}
\renewcommand{\leq}{\ensuremath{\leqslant}}
\renewcommand{\ge}{\ensuremath{\geqslant}}
\renewcommand{\geq}{\ensuremath{\geqslant}}
\renewcommand{\emptyset}{\varnothing}

%%% Дополнительная работа с математикой
\usepackage{amsmath,amsfonts,amssymb,amsthm,mathtools} % AMS
\usepackage{icomma} % "Умная" запятая: $0,2$ --- число, $0, 2$ --- перечисление

%% Номера формул
%\mathtoolsset{showonlyrefs=true} % Показывать номера только у тех формул, на которые есть \eqref{} в тексте.
%\usepackage{leqno} % Нумереация формул слева

%% Свои команды
\DeclareMathOperator{\sgn}{\mathop{sgn}}

%% Перенос знаков в формулах (по Львовскому)
\newcommand*{\hm}[1]{#1\nobreak\discretionary{}
	{\hbox{$\mathsurround=0pt #1$}}{}}

%%% Работа с картинками
\usepackage{graphicx}  % Для вставки рисунков
\setlength\fboxsep{3pt} % Отступ рамки \fbox{} от рисунка
\setlength\fboxrule{1pt} % Толщина линий рамки \fbox{}
\usepackage{wrapfig} % Обтекание рисунков текстом

%%% Работа с таблицами
\usepackage{array,tabularx,tabulary,booktabs} % Дополнительная работа с таблицами
\usepackage{longtable}  % Длинные таблицы
\usepackage{multirow} % Слияние строк в таблице

%%% Теоремы
\theoremstyle{plain} % Это стиль по умолчанию, его можно не переопределять.
\newtheorem{theorem}{Теорема}[section]
\newtheorem{proposition}[theorem]{Утверждение}

\theoremstyle{definition} % "Определение"
\newtheorem{corollary}{Следствие}[theorem]
\newtheorem{problem}{Задача}[section]

\theoremstyle{remark} % "Примечание"
\newtheorem*{nonum}{Решение}

%%% Программирование
\usepackage{etoolbox} % логические операторы

%%% Страница
\usepackage{extsizes} % Возможность сделать 14-й шрифт
\usepackage{geometry} % Простой способ задавать поля
\geometry{top=25mm}
\geometry{bottom=35mm}
\geometry{left=35mm}
\geometry{right=20mm}
%
%\usepackage{fancyhdr} % Колонтитулы
% 	\pagestyle{fancy}
%\renewcommand{\headrulewidth}{0pt}  % Толщина линейки, отчеркивающей верхний колонтитул
% 	\lfoot{Нижний левый}
% 	\rfoot{Нижний правый}
% 	\rhead{Верхний правый}
% 	\chead{Верхний в центре}
% 	\lhead{Верхний левый}
%	\cfoot{Нижний в центре} % По умолчанию здесь номер страницы

\usepackage{setspace} % Интерлиньяж
%\onehalfspacing % Интерлиньяж 1.5
%\doublespacing % Интерлиньяж 2
%\singlespacing % Интерлиньяж 1

\usepackage{lastpage} % Узнать, сколько всего страниц в документе.

\usepackage{soul} % Модификаторы начертания

\usepackage{hyperref}
\usepackage[usenames,dvipsnames,svgnames,table,rgb]{xcolor}
\hypersetup{				% Гиперссылки
	unicode=true,           % русские буквы в раздела PDF
	pdftitle={Заголовок},   % Заголовок
	pdfauthor={Автор},      % Автор
	pdfsubject={Тема},      % Тема
	pdfcreator={Создатель}, % Создатель
	pdfproducer={Производитель}, % Производитель
	pdfkeywords={keyword1} {key2} {key3}, % Ключевые слова
	colorlinks=true,       	% false: ссылки в рамках; true: цветные ссылки
	linkcolor=red,          % внутренние ссылки
	citecolor=black,        % на библиографию
	filecolor=magenta,      % на файлы
	urlcolor=cyan           % на URL
}

\usepackage{csquotes} % Еще инструменты для ссылок

% \usepackage[style=authoryear,maxcitenames=2,backend=biber,sorting=nty]{biblatex}
% \addbibresource{references.bib}
\usepackage[numbers]{natbib}

\usepackage{multicol} % Несколько колонок

\usepackage{tikz} % Работа с графикой
\usepackage{pgfplots}
\usepackage{pgfplotstable}


\author{Eduard Vladimirov, Daniil Kazachkov, Vadim Strijov}
\title{\textbf{Cross-attention and CCA in EEG}}
\date{\today}

\begin{document}
	\maketitle
	
	\section{Abstract}
        На сегодняшний день работа с мультимодальными данными набирает всё большую популярность: учет взаимосвязей между ними улучшает качество предсказания. В этой статье мы предлагаем новую архитектуру, использующую преимущества алгоритма Canonical Correlation Analysis (CCA) и механизма Attention. Ниже будет показано, что CCA - частный случай Attention, а значит, мультимодальность можно встроить внутрь фреймворка Attention. Работа полученной модели иллюстрируется на задаче классификации удара теннисного мяча по датасету Real World Table Tennis. 

        $\mathbf{keywords:}$ CCA, Attention, BCI, online-classification.

        \section{Introduction}
        Современные задачи, связанные с обработкой и анализом данных, всё чаще включают несколько разнородных источников информации, объединенных в одну мультимодальную систему. Подтверждения этому можно найти в здравоохранении, аффективных вычислениях, роботехнических системах, образовании \citep{paulpu2020foundation}. Разнородность данных приводит к необходимости в их эффективной обработке, а значит, совершенствованию методов представления, выравнивания, обобщении и генерации данных \citep{paulpu2020foundation}. Таким образом, разработка подходов, способных справляться с многообразием структур и типов информации, становится ключевым направлением в области анализа данных и машинного обучения. \\
        Мультимодальность - мощный инструмент для улучшения качества ответов модели \citep{huang2021whatmakes}. Канонический корреляционный анализ (CCA) \citep{yang2021surveycca} является очень популярным статистическим методом, снижения размерности двух множеств данных, при котором корреляция между парными переменными в общем подпространстве взаимно максимизируется. В таких работах как \citep{chandar2016corrnet}, \citep{bayoudh2022surveydmlcv} авторы показали, что он улучшает качество в задачах сопоставления событий. Однако CCA может моделировать лишь линейные зависимости. \\
        Существует несколько подходов по улучшению CCA: Kernel-CCA, Correlaton Neural Network и Deep-CCA. В таблице (\ref{tab:methods_compar1}) приведен небольшой анализ этих подходов.
        
        \begin{table}[h!]
        \centering
        \begin{tabularx}{\textwidth}{|p{3cm}|X|} % X adjusts columns to fit the page
        \hline
        \textbf{Метод} & \textbf{Особенность} \\ \hline
        Kernel CCA &
        \begin{itemize}
        \item стандартное расширение CCA на поиск нелинейных зависимостей
            \item хорошая обработка высокоразмерных данных \citep{biebann2010tempkernelcca};
            \item скрытое представление ограничено фиксированным ядром;
            \item является непараметрическим методом, что приводит к худшему масштабированию на новые данные \citep{andrew2013deepcca}.
        \end{itemize} \\ \hline
        Correlation Neural Network \citep{chandar2016corrnet} &
        \begin{itemize}
            \item подход на основе Autoencoders, который явно максимизирует корреляцию между представлениями при проецировании на общее подпространство.
        \end{itemize} \\ \hline
        \end{tabularx}
        \caption{Сравнение подходов}
        \label{tab:methods_compar1}
        \end{table}
        
        \begin{table}[h!]
        \centering
        \begin{tabularx}{\textwidth}{|p{3cm}|X|} % X adjusts columns to fit the page
        \hline
        \textbf{Метод} & \textbf{Особенность} \\ \hline
        Deep CCA \citep{andrew2013deepcca} & 
        \begin{itemize}
            \item подход на основе Autoencoders, который явно максимизирует корреляцию между представлениями при проецировании на общее подпространство;
            \item не считает скалярное произведение (inner product), что позволяет лучше масштабировать модель на новые данные.
        \end{itemize} \\ \hline
        \end{tabularx}
        \caption{Сравнение подходов}
        \label{tab:methods_compar2}
        \end{table} 

        В нашей работе мы рассмотрим улучшение CCA, связанное с механизмом Attention - нелинейным преобразованием последовательности для поиска сложных зависимостей. Так, усовершенствование cross-attention позволит лучше отсеивать информацию, снизит размерность пространства и тем самым повысит качество прогноза. \\
        Ставя перед собой цель использовать преимущества обоих методов, мы представляем модель CCT: Canonical-Correlation Transformer. Архитектура у нее следующая: из пакета PyRiemann (мб заменим на CNN EEGNet) [ссылка] мы берем энкодер и преобразуем поданный на вход ЭЭГ сигнал в скрытое пространство. Далее в качестве механизма внимания используем ... (дополнить, когда станет понятно). Выход модели - вероятность события принадлежать одному из четырех классов: победному, нейтральному или проигрышному удару. \\    
        Наша задача: как по данным ЭЭГ игрока в настольный теннис в режиме реального времени классифицировать момент удара - типичный пример из области Brain-Computer Interface (BCI), когда необходимо эффективно работать с данными разных модальностей и классифицировать события в онлайн режиме. В качестве датасета мы взяли предобработанные данные из "Real World Table Tennis" \cite{amanda2024dataset}.\\

        \section{Related Works}
        Интерфейс мозг-компьютер — это система, которая измеряет активность головного мозга и преобразует ее в (приблизительно) реальном времени в функционально полезные выходные сигналы для замены, восстановления, усиления, дополнения и/или улучшения естественных выходных мозговых сигналов, тем самым изменяя текущие процессы взаимодействия между мозгом и его внешней или внутренней средой [BCI Society]. Одним из наиболее распространенных неинвазивных методов получения информации об электрической активности мозга является ЭЭГ. Умение эффективно работать с этим типом данных полезно во многих задачах: распознавании эмоций [EEG-Based Emotion Recognition via Convolutional Transformer with Class Confusion-Aware Attention], прогнозировании рецедивов болезни [https://arxiv.org/abs/1801.07936], компенсации серьезных двигательных нарушений [https://iopscience.iop.org/article/10.1088/1741-2552/aab2f2/pdf] и т.д. \\
        \textbf{написать про существующие attention cca подходы, см [https://escholarship.org/uc/item/21p105jn]}        

        \section{Problem Statement}
        \subsection{CCT: Canonical-Correlation Transformer}
        Мы продолжаем расширять область применимости CCA и представляем способ встраивания его в Attention.\\
        Canonical Correlation Analysis (CCA) - стандартный инструмент для выявления линейных зависимостей между двумя наборами данных [Canonical correlation analysis: An overview with application to learning methods]. Пусть нам дано множество векторов $X\in \dR^{n_1\times m}$ и $Y\in \dR^{n_2\times m}$, где $m$ - количество векторов. Задача CCA - найти такие афинные преобразования $\bW_x, \bW_y$, которые максимизируют корреляцию между $X, Y$ в новом пространстве:
        \begin{equation}
            \begin{aligned}
            \bW^{*}_x, \bW^{*}_y &= \arg\max_{\bW_x, \bW_y} \, \text{corr}(\bW_x\T X, \bW_y\T Y) \\
            &= \arg\max_{\bW_x, \bW_y}\frac{\bW_x\T \hat{\bE}[XY\T] \bW_y}{\sqrt{\bW_x\T \hat{\bE}[XX\T] \bW_x \bW_y\T \hat{\bE}[YY\T] \bW_y}} \\
            &= \arg\max_{\bW_x, \bW_y}\frac{\bW_x\T C_{12} \bW_y}{\sqrt{\bW_x\T C_{11} \bW_x \bW_y\T C_{22} \bW_y}},
            \end{aligned}
        \end{equation}
        где $\hat{\bE}[f(\bx, \by)] = \frac{1}{m}\sum\limits_{i=1}^{m}f(\bx_i, \by_i)$, матрицы ковариации $X$ и $Y$ есть $C_{11} = \frac{1}{m}XX\T \in \dR^{n_1\times n_1}$, $C_{22} = \frac{1}{m}YY\T \in \dR^{n_2\times n_2}$, а матрица кросс-ковариации X, Y есть $C_{12} = \frac{1}{m}XY\T \in\dR^{n_1\times n_2}$.

        Развивая идею [Learning Relationships between Text, Audio, and Video via Deep Canonical Correlation for Multimodal Language Analysis] для решения воспользуемся методом Singular Value Decomposition (SVD, Martin and Maes 1979) для $Z = C_{11}^{-1/2}C_{12}C_{22}^{-1/2}$ и получим матрицы U, S, V. Тогда
        \begin{equation}
            \begin{aligned}
                &\bW_x^* = C_{11}^{-\frac{1}{2}}U = (\frac{1}{m}XX\T)^{-\frac{1}{2}}U \\
                &\bW_y^* = C_{22}^{-\frac{1}{2}}V = (\frac{1}{m}YY\T)^{-\frac{1}{2}}V \\
                &\text{corr}(\bW_x\T^* X, \bW_y\T^* Y) = \text{trace}(Z\T Z)^{\frac{1}{2}}
            \end{aligned}
        \end{equation}

        В таких работах как [Learning Relationships between Text, Audio, and Video via Deep Canonical Correlation for Multimodal Language Analysis], [Deep Canonical Correlation Analysis] раскрыт подход использования глубоких сетей для обучения нелинейных преобразований двух наборов данных в пространство, в котором данные сильно скоррелированы. Мы же рассмотрим механизм внимания [Neural machine translation by jointly learning to align and translate], который используется для определения важности разных частей входных данных. 

        Механизм самовнимания определяется следующим образом:
	
	\begin{equation}
		\begin{aligned}
			&\text{attn}: \mathbb{R}^{m \times d} \times \mathbb{R}^{m \times d} \times \mathbb{R}^{m \times d} \longrightarrow \mathbb{R}^{m \times d} \\
			&\text{attn}(Q, K, V) = \varphi\left(\frac{Q K^\top}{\sqrt{d}}\right) V
		\end{aligned}
		\label{attn}
	\end{equation}
	
        where $Q, K, V \in \dR^{m \times d}$ represent the queries, keys, and values, respectively, and $\varphi: \dR^{m \times m} \longrightarrow \dR^{m \times m}$ is row-wise applied nonlinear function, usually softmax.
	
        Self-attention applied to the input $X \in \dR^{m \times n_1}$ is computed as:
	
	\begin{equation}
		\begin{aligned}
			&\text{self-attn}: \mathbb{R}^{m \times n_1} \longrightarrow \mathbb{R}^{m \times d} \\
			&\text{self-attn}(X) = \text{attn}(X W_q, X W_k, X W_v)
		\end{aligned}
		\label{self-attn}
	\end{equation}

	where $W_q, W_k, W_v \in \dR^{n_1 \times d}$ ~--- parameter matrices
	
        In multihead attention, several attention heads are used in parallel, where each head computes its own attention weights and outputs. The outputs are then concatenated and linearly transformed by a weight matrix $W^Q \in \dR^{p \cdot d \times d}$:
	
	\begin{equation}
		\text{multihead-attn}(Q, K, V) = [\text{head}_1, \ldots, \text{head}_p] W^Q,
		\label{multihead-attn}
	\end{equation}
	
	where $\text{head}_i = \text{self-attn}(X)$
	
	Cross-attention, in contrast, involves attention between two different sets of inputs. It computes attention by using one set of inputs for queries $X_1 \in \dR^{m \times d_1}$ and another set for keys and values $X_2 \in \dR^{m \times d_2}$:
	
	\begin{equation}
		\text{cross-attn}(X_1, X_2) = \text{attn}(X_1 W_q, X_2 W_k, X_2 W_v) \label{cross-attn}
	\end{equation}

        \section*{CCA and attention}
	
	Both CCA and attention mechanisms aim to find relationships between two sets of data. However, they differ significantly in their approach and applications:
	
	\setlength{\extrarowheight}{3mm}
	
	\begin{table}[bhtp]
		\centering
		\begin{tabulary}{\textwidth}{|C|C|C|}
			\hline
			\textbf{Aspect} & \textbf{Attention} & \textbf{Canonical Correlation Analysis (CCA)} \\ 
			\hline
			Goal & Identify relevant parts of input sequences & Receive embeddings in the same hidden space + dimensionality reduction \\
			\hline
			Similarity Measure & $A = \frac{1}{\sqrt{d}} Q K\T$ ~-- attention matrix & $\text{tr}(A\T S_{12} B), \text{ s.t. } A\T S_{11} A = B\T S_{22} B = I$ \\
			\hline
			Optimization Goal & Minimize task-specific loss & $\max_{A,B} \text{corr}(A^T X, B^T Y)$ \\
			\hline
		\end{tabulary}
		\caption{Comparison of Attention Mechanisms and CCA}
	\end{table}
	
	Note that $A\T S_{12} B = \frac{1}{m} A\T X Y\T B = \frac{1}{m} A\T X \left( B\T Y \right)\T = \frac{1}{m} \widehat{Q} \widehat{K}\T $. And it's quite similar to attention matrix formula $A = \dfrac{1}{\sqrt{d}} Q K\T$. Especially, in cross attention case, where $Q$ is a linear transformation of $X_1$ and $K$ is a linear transformation of $X_2$:
	
	\begin{table}[bhtp]
		\centering
		\begin{tabulary}{\textwidth}{|C|C|C|C|C|C|}
			\hline
			\textbf{Attn} & \textbf{Self-attn} & \textbf{Cross-attn} & \textbf{CCA} & \textbf{CCA-X} & \textbf{CCA-Y} \\
			\hline
			$Q$                & $W_Q\T X$                 & $W_Q\T X$                  & $A\T X$ & $S_{11}^{-\frac{1}{2}}X$ & $S_{11}^{-\frac{1}{2}}X$  \\
			\hline
			$K$                & $W_K\T X$                 & $W_K\T Y$                  & $B\T Y$ & $S_{22}^{-\frac{1}{2}}Y$ & $S_{22}^{-\frac{1}{2}}Y$    \\
			\hline
			$V$                & $W_V\T X$                 & $W_V\T Y$                  & I & $S_{11}^{-\frac{1}{2}}X$ & $S_{22}^{-\frac{1}{2}}Y$  \\
			\hline 
			$\varphi$ & softmax & softmax & Id & $\text{SVD}_U$ & $\text{SVD}_V$ \\
			\hline  
		\end{tabulary}
		\caption{United notation of CCA and attention}
	\end{table}

	Let's view in detail the CCA projection of $X$ to latent space:
	\begin{equation}
		\begin{aligned}
			\text{CCA}_{XY}(X) &= U\T S_{11}^{-\frac{1}{2}}X = U\T X_1 \\
			\text{CCA}_{XY}(Y) &= V\T S_{22}^{-\frac{1}{2}}Y = V\T Y_1 \\
			Z &= S_{11}^{- \frac{1}{2}} S_{12} S_{22}^{- \frac{1}{2}} = \frac{1}{m} X_1 Y_1\T
		\end{aligned}
	\end{equation}
        

        \section{Experiments}
        [какие-то общие слова, если нужны] Например, про то, что пробовали PyRiemann и трансформер из braincode.
        \subsection{Dataset Details}
        \begin{itemize}
            \item что за датасет
            \item какая предобработка данных проводилась
            \item в каком виде данные подавались в модель
        \end{itemize}
        Мы оцениваем производительность модели на датасете "Real world table-tennis" [Dual-layer electroencephalography data during real-world table tennis]. Датасет включает в себя 
        
        \subsection{Training Details}
        Если возникнут какие-то проблемы или эвристики при обучении, то пишем сюда
        \begin{itemize}
            \item На каких параметрах обучали сетку, 
        \end{itemize}

        \subsection{Experimental Results}
        Получилась вот такая точность и почему.

        \section{Appendix}
        \subsection{CCA-Attention table}
        Приведем пример вывода значений таблицы для одного примера:
	\nocite{*}
        \bibliographystyle{unsrt} % Use a numbered bibliography style
        \bibliography{references.bib}
	% \printbibliography
	
\end{document}
