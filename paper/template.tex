\documentclass[a4paper,14pt]{article}

%%% Работа с русским языком
\usepackage{cmap}					% поиск в PDF
\usepackage{mathtext} 				% русские буквы в формулах
\usepackage[T2A]{fontenc}			% кодировка
\usepackage[utf8]{inputenc}			% кодировка исходного текста
\usepackage[english,russian]{babel}	% локализация и переносы
\usepackage{indentfirst}
\frenchspacing

\newcommand{\bz}{\mathbf{z}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bp}{\mathbf{p}}
\newcommand{\bq}{\mathbf{q}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bZ}{\mathbf{Z}}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\bH}{\mathbf{H}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\bP}{\mathbf{P}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bC}{\mathbf{C}}
\newcommand{\bE}{\mathbf{E}}
\newcommand{\bF}{\mathbf{F}}
\newcommand{\bomega}{\boldsymbol{\omega}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bgamma}{\boldsymbol{\gamma}}
\newcommand{\bdelta}{\boldsymbol{\delta}}
\newcommand{\bPsi}{\boldsymbol{\Psi}}
\newcommand{\bpsi}{\boldsymbol{\psi}}
\newcommand{\bxi}{\boldsymbol{\xi}}
\newcommand{\bchi}{\boldsymbol{\chi}}
\newcommand{\bzeta}{\boldsymbol{\zeta}}
\newcommand{\blambda}{\boldsymbol{\lambda}}
\newcommand{\beps}{\boldsymbol{\varepsilon}}
\newcommand{\bZeta}{\boldsymbol{Z}}
% mathcal
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cW}{\mathcal{W}}

\newcommand{\dH}{\mathbb{H}}
\newcommand{\dR}{\mathbb{R}}
\newcommand{\dE}{\mathbb{E}}
% transpose
\newcommand{\T}{^{\mathsf{T}}}

\renewcommand{\epsilon}{\ensuremath{\varepsilon}}
\renewcommand{\phi}{\ensuremath{\varphi}}
\renewcommand{\kappa}{\ensuremath{\varkappa}}
\renewcommand{\le}{\ensuremath{\leqslant}}
\renewcommand{\leq}{\ensuremath{\leqslant}}
\renewcommand{\ge}{\ensuremath{\geqslant}}
\renewcommand{\geq}{\ensuremath{\geqslant}}
\renewcommand{\emptyset}{\varnothing}

%%% Дополнительная работа с математикой
\usepackage{amsmath,amsfonts,amssymb,amsthm,mathtools} % AMS
\usepackage{icomma} % "Умная" запятая: $0,2$ --- число, $0, 2$ --- перечисление

%% Номера формул
%\mathtoolsset{showonlyrefs=true} % Показывать номера только у тех формул, на которые есть \eqref{} в тексте.
%\usepackage{leqno} % Нумереация формул слева

%% Свои команды
\DeclareMathOperator{\sgn}{\mathop{sgn}}

%% Перенос знаков в формулах (по Львовскому)
\newcommand*{\hm}[1]{#1\nobreak\discretionary{}
	{\hbox{$\mathsurround=0pt #1$}}{}}

%%% Работа с картинками
\usepackage{graphicx}  % Для вставки рисунков
\setlength\fboxsep{3pt} % Отступ рамки \fbox{} от рисунка
\setlength\fboxrule{1pt} % Толщина линий рамки \fbox{}
\usepackage{wrapfig} % Обтекание рисунков текстом

%%% Работа с таблицами
\usepackage{array,tabularx,tabulary,booktabs} % Дополнительная работа с таблицами
\usepackage{longtable}  % Длинные таблицы
\usepackage{multirow} % Слияние строк в таблице

%%% Теоремы
\theoremstyle{plain} % Это стиль по умолчанию, его можно не переопределять.
\newtheorem{theorem}{Теорема}[section]
\newtheorem{proposition}[theorem]{Утверждение}

\theoremstyle{definition} % "Определение"
\newtheorem{corollary}{Следствие}[theorem]
\newtheorem{problem}{Задача}[section]

\theoremstyle{remark} % "Примечание"
\newtheorem*{nonum}{Решение}

%%% Программирование
\usepackage{etoolbox} % логические операторы

%%% Страница
\usepackage{extsizes} % Возможность сделать 14-й шрифт
\usepackage{geometry} % Простой способ задавать поля
\geometry{top=25mm}
\geometry{bottom=35mm}
\geometry{left=35mm}
\geometry{right=20mm}
%
%\usepackage{fancyhdr} % Колонтитулы
% 	\pagestyle{fancy}
%\renewcommand{\headrulewidth}{0pt}  % Толщина линейки, отчеркивающей верхний колонтитул
% 	\lfoot{Нижний левый}
% 	\rfoot{Нижний правый}
% 	\rhead{Верхний правый}
% 	\chead{Верхний в центре}
% 	\lhead{Верхний левый}
%	\cfoot{Нижний в центре} % По умолчанию здесь номер страницы

\usepackage{setspace} % Интерлиньяж
%\onehalfspacing % Интерлиньяж 1.5
%\doublespacing % Интерлиньяж 2
%\singlespacing % Интерлиньяж 1

\usepackage{lastpage} % Узнать, сколько всего страниц в документе.

\usepackage{soul} % Модификаторы начертания

\usepackage{hyperref}
\usepackage[usenames,dvipsnames,svgnames,table,rgb]{xcolor}
\hypersetup{				% Гиперссылки
	unicode=true,           % русские буквы в раздела PDF
	pdftitle={Заголовок},   % Заголовок
	pdfauthor={Автор},      % Автор
	pdfsubject={Тема},      % Тема
	pdfcreator={Создатель}, % Создатель
	pdfproducer={Производитель}, % Производитель
	pdfkeywords={keyword1} {key2} {key3}, % Ключевые слова
	colorlinks=true,       	% false: ссылки в рамках; true: цветные ссылки
	linkcolor=red,          % внутренние ссылки
	citecolor=black,        % на библиографию
	filecolor=magenta,      % на файлы
	urlcolor=cyan           % на URL
}

\usepackage{csquotes} % Еще инструменты для ссылок

% \usepackage[style=authoryear,maxcitenames=2,backend=biber,sorting=nty]{biblatex}
% \addbibresource{references.bib}
\usepackage[numbers]{natbib}

\usepackage{multicol} % Несколько колонок

\usepackage{tikz} % Работа с графикой
\usepackage{pgfplots}
\usepackage{pgfplotstable}


\author{Eduard Vladimirov, Daniil Kazachkov, Vadim Strijov}
\title{\textbf{Cross-attention and CCA in EEG}}
\date{\today}

\begin{document}
	\maketitle
	
	\section{Abstract}
        На сегодняшний день работа с мультимодальными данными набирает всё большую популярность: учет взаимосвязей между ними улучшает качество предсказания. В этой статье мы предлагаем новую архитектуру, использующую преимущества алгоритма Canonical Correlation Analysis (CCA) и механизма Attention. Ниже будет показано, что CCA - частный случай Attention, а значит, мультимодальность можно встроить внутрь фреймворка Attention. Работа полученной модели иллюстрируется на задаче классификации удара теннисного мяча по датасету Real World Table Tennis. 

        $\mathbf{keywords:}$ CCA, Attention, BCI, online-classification.

        \section{Introduction}

        Мультимодальность - мощный инструмент для улучшения качества ответов модели [What Makesz Multi-modal Learning Better than Single]. Канонический корреляционный анализ (CCA) [A Survey on Canonical Correlation Analysis] является очень популярным статистическим методом, снижения размерности двух множеств данных, при котором корреляция между парными переменными в общем подпространстве взаимно максимизируется. В таких работах как [Correlational Neural Networks], [A survey on deep multimodal learning for computer vision: advances, trends, applications, and datasets] авторы показали, что он улучшает качество в задачах сопоставления событий. Однако CCA может моделировать лишь линейные зависимости. \\
        Существует несколько подходов по улучшению CCA, например, Kernel-CCA и Deep-CCA, каждый из которых имеет свои преимущества.  
        \textbf{Тут вставка про такие улучшения, как Kernel CCA, Deep CCA... и их показатели}.\\
        В противоположность этому, механизм Attention находит сложные, нелинейные зависимости. Усовершенствование cross-attention позволит лучше отсеивать информацию, снизит размерность пространства и тем самым ускорит вычисления. \\
        Ставя перед собой цель использовать преимущества каждого метода, мы представляем модель CCT: Canonical-Correlation Transformer. Архитектура у нее следующая: из пакета PyRiemann (мб заменим на CNN EEGNet) [ссылка] мы берем энкодер и преобразуем поданный на вход ЭЭГ сигнал в скрытое пространство. Далее в качестве механизма внимания используем ... (дополнить, когда станет понятно). Выход модели - вероятность события принадлежать одному из четырех классов: попадание по мячу (1), нейтральное событие (0), промах по мячу (-1), попадание, после которого успешных ударов не было (2). \\    
        Наша задача: как по данным ЭЭГ игрока в настольный теннис в режиме реального времени классифицировать момент удара - типичный пример из области Brain-Computer Interface (BCI), когда необходимо эффективно работать с данными разных модальностей и классифицировать события в онлайн режиме. В качестве датасета мы взяли предобработанные данные из "Real World Table Tennis" \cite{amanda2024dataset}.\\

        \subsection{Related Works}
        \begin{itemize}
            \item про нейрокомпьютерный интерфейс
            \item про онлайн-классификацию
            \item про emotional recognition с attnetion cca
            \item мб про аналогичные оффлайн-задачи
        \end{itemize}
        Нейрокомпьютерный интерфейс (Brain-Computer Interface, BCI) \cite{shih2012brain} считывает сигналы поверхности кортекса головного мозга, анализирует и переводит в команды исполняющей системы. Результатом измерений является временной ряд напряжений на электродах, который используется в задаче декодирования сигнала. \\
        Одним из наиболее распространенных неинвазивных методов получения информации об электрической активности мозга является ЭЭГ. Умение эффективно работать с подобными данными полезно во многих задачах: распознавание эмоций [ссылкка], прогнозирование рецедивов болезни [ссылка], анализ сна [ссылка] и других.
        В этих примерах прослеживается и другая важная область: онлайн-классификация или stream-processing. 

        В этой работе авторы делали вот это.., использовали вот это. Явные методы, почему именно их, а не другие.

        \subsection{problem statement}
    
        \section{CCT: Canonical-Correlation Transformer}
        \subsection{Model}
        Мы продолжаем расширять область применимости CCT и представляем способ встраивания его в Attention.
        \begin{itemize}
            \item отдельно CCA
            \item отдельно Attention
            \item связь CCA + Attention (у Эдуарда из файла взять)
        \end{itemize}
        Canonical Correlation Analysis (CCA) - стандартный инструмент для выявления линейных зависимостей между двумя наборами данных [Canonical correlation analysis: An overview with application to learning methods]. Пусть нам дано множество векторов $X\in \dR^{n_1\times m}$ и $Y\in \dR^{n_2\times m}$, где $m$ - количество векторов. Задача CCA - найти такие афинные преобразования $\bW_x, \bW_y$, которые максимизируют корреляцию между $X, Y$ в новом пространстве:
        \begin{equation}
            \begin{aligned}
            \bW^{*}_x, \bW^{*}_y &= \arg\max_{\bW_x, \bW_y} \, \text{corr}(\bW_x\T X, \bW_y\T Y) \\
            &= \arg\max_{\bW_x, \bW_y}\frac{\bW_x\T \hat{\bE}[XY\T] \bW_y}{\sqrt{\bW_x\T \hat{\bE}[XX\T] \bW_x \bW_y\T \hat{\bE}[YY\T] \bW_y}} \\
            &= \arg\max_{\bW_x, \bW_y}\frac{\bW_x\T C_{12} \bW_y}{\sqrt{\bW_x\T C_{11} \bW_x \bW_y\T C_{22} \bW_y}},
            \end{aligned}
        \end{equation}
        где $\hat{\bE}[f(\bx, \by)] = \frac{1}{m}\sum\limits_{i=1}^{m}f(\bx_i, \by_i)$, матрицы ковариации $X$ и $Y$ есть $C_{11} = \frac{1}{m}XX\T \in \dR^{n_1\times n_1}$, $C_{22} = \frac{1}{m}YY\T \in \dR^{n_2\times n_2}$, а матрица кросс-ковариации X, Y есть $C_{12} = \frac{1}{m}XY\T \in\dR^{n_1\times n_2}$.

        Развивая идею [Learning Relationships between Text, Audio, and Video via Deep Canonical Correlation for Multimodal Language Analysis] для решения воспользуемся методом Singular Value Decomposition (SVD, Martin and Maes 1979) для $Z = C_{11}^{-1/2}C_{12}C_{22}^{-1/2}$ и получим матрицы U, S, V. Тогда
        \begin{equation}
            \begin{aligned}
                &\bW_x^* = C_{11}^{-\frac{1}{2}}U = (\frac{1}{m}XX\T)^{-\frac{1}{2}}U \\
                &\bW_y^* = C_{22}^{-\frac{1}{2}}V = (\frac{1}{m}YY\T)^{-\frac{1}{2}}V \\
                &\text{corr}(\bW_x\T^* X, \bW_y\T^* Y) = \text{trace}(Z\T Z)^{\frac{1}{2}}
            \end{aligned}
        \end{equation}

        В таких работах как [Learning Relationships between Text, Audio, and Video via Deep Canonical Correlation for Multimodal Language Analysis], [Deep Canonical Correlation Analysis] раскрыт подход использования глубоких сетей для обучения нелинейных преобразований двух наборов данных в пространство, в котором данные сильно скоррелированы. Мы же рассмотрим механизм внимания [Neural machine translation by jointly learning to align and translate], который используется для определения важности разных частей входных данных. 

        Механизм самовнимания определяется следующим образом:
	
	\begin{equation}
		\begin{aligned}
			&\text{attn}: \mathbb{R}^{m \times d} \times \mathbb{R}^{m \times d} \times \mathbb{R}^{m \times d} \longrightarrow \mathbb{R}^{m \times d} \\
			&\text{attn}(Q, K, V) = \varphi\left(\frac{Q K^\top}{\sqrt{d}}\right) V
		\end{aligned}
		\label{attn}
	\end{equation}
	
        where $Q, K, V \in \dR^{m \times d}$ represent the queries, keys, and values, respectively, and $\varphi: \dR^{m \times m} \longrightarrow \dR^{m \times m}$ is row-wise applied nonlinear function, usually softmax. The dot product between $Q$ and $K$ determines the attention weights, which are normalized using the softmax function. The result is then applied to the values $V$ to generate the output.
	
        Self-attention applied to the input $X \in \dR^{m \times n_1}$ is computed as:
	
	\begin{equation}
		\begin{aligned}
			&\text{self-attn}: \mathbb{R}^{m \times n_1} \longrightarrow \mathbb{R}^{m \times d} \\
			&\text{self-attn}(X) = \text{attn}(X W_q, X W_k, X W_v)
		\end{aligned}
		\label{self-attn}
	\end{equation}

	where $W_q, W_k, W_v \in \dR^{n_1 \times d}$ ~--- parameter matrices
	
        In multihead attention, several attention heads are used in parallel, where each head computes its own attention weights and outputs. The outputs are then concatenated and linearly transformed by a weight matrix $W^Q \in \dR^{p \cdot d \times d}$:
	
	\begin{equation}
		\text{multihead-attn}(Q, K, V) = [\text{head}_1, \ldots, \text{head}_p] W^Q,
		\label{multihead-attn}
	\end{equation}
	
	where $\text{head}_i = \text{self-attn}(X)$
	
	Cross-attention, in contrast, involves attention between two different sets of inputs. It computes attention by using one set of inputs for queries $X_1 \in \dR^{m \times d_1}$ and another set for keys and values $X_2 \in \dR^{m \times d_2}$:
	
	\begin{equation}
		\text{cross-attn}(X_1, X_2) = \text{attn}(X_1 W_q, X_2 W_k, X_2 W_v) \label{cross-attn}
	\end{equation}

        \section*{CCA and attention}
	
	Both CCA and attention mechanisms aim to find relationships between two sets of data. However, they differ significantly in their approach and applications:
	
	\setlength{\extrarowheight}{3mm}
	
	\begin{table}[bhtp]
		\centering
		\begin{tabulary}{\textwidth}{|C|C|C|}
			\hline
			\textbf{Aspect} & \textbf{Attention} & \textbf{Canonical Correlation Analysis (CCA)} \\ 
			\hline
			Goal & Identify relevant parts of input sequences & Receive embeddings in the same hidden space + dimensionality reduction \\
			\hline
			Similarity Measure & $A = \frac{1}{\sqrt{d}} Q K\T$ ~-- attention matrix & $\text{tr}(A\T S_{12} B), \text{ s.t. } A\T S_{11} A = B\T S_{22} B = I$ \\
			\hline
			Optimization Goal & Minimize task-specific loss & $\max_{A,B} \text{corr}(A^T X, B^T Y)$ \\
			\hline
		\end{tabulary}
		\caption{Comparison of Attention Mechanisms and CCA}
	\end{table}
	
	Note that $A\T S_{12} B = \frac{1}{m} A\T X Y\T B = \frac{1}{m} A\T X \left( B\T Y \right)\T = \frac{1}{m} \widehat{Q} \widehat{K}\T $. And it's quite similar to attention matrix formula $A = \dfrac{1}{\sqrt{d}} Q K\T$. Especially, in cross attention case, where $Q$ is a linear transformation of $X_1$ and $K$ is a linear transformation of $X_2$:
	
	\begin{table}[bhtp]
		\centering
		\begin{tabulary}{\textwidth}{|C|C|C|C|C|C|}
			\hline
			\textbf{Attn} & \textbf{Self-attn} & \textbf{Cross-attn} & \textbf{CCA} & \textbf{CCA-X} & \textbf{CCA-Y} \\
			\hline
			$Q$                & $W_Q\T X$                 & $W_Q\T X$                  & $A\T X$ & $S_{11}^{-\frac{1}{2}}X$ & $S_{11}^{-\frac{1}{2}}X$  \\
			\hline
			$K$                & $W_K\T X$                 & $W_K\T Y$                  & $B\T Y$ & $S_{22}^{-\frac{1}{2}}Y$ & $S_{22}^{-\frac{1}{2}}Y$    \\
			\hline
			$V$                & $W_V\T X$                 & $W_V\T Y$                  & I & $S_{11}^{-\frac{1}{2}}X$ & $S_{22}^{-\frac{1}{2}}Y$  \\
			\hline 
			$\varphi$ & softmax & softmax & Id & $\text{SVD}_U$ & $\text{SVD}_V$ \\
			\hline  
		\end{tabulary}
		\caption{United notation of CCA and attention}
	\end{table}

	Let's view in detail the CCA projection of $X$ to latent space:
	\begin{equation}
		\begin{aligned}
			\text{CCA}_{XY}(X) &= U\T S_{11}^{-\frac{1}{2}}X = U\T X_1 \\
			\text{CCA}_{XY}(Y) &= V\T S_{22}^{-\frac{1}{2}}Y = V\T Y_1 \\
			Z &= S_{11}^{- \frac{1}{2}} S_{12} S_{22}^{- \frac{1}{2}} = \frac{1}{m} X_1 Y_1\T
		\end{aligned}
	\end{equation}
        

        \section{Experiments}
        [какие-то общие слова, если нужны] Например, про то, что пробовали PyRiemann и трансформер из braincode.
        \subsection{Dataset Details}
        \begin{itemize}
            \item что за датасет
            \item какая предобработка данных проводилась
            \item в каком виде данные подавались в модель
        \end{itemize}
        
        \subsection{Training Details}
        Если возникнут какие-то проблемы или эвристики при обучении, то пишем сюда
        \begin{itemize}
            \item На каких параметрах обучали сетку, 
        \end{itemize}

        \subsection{Experimental Results}
        Получилась вот такая точность и почему.

        \section{Appendix}
        \subsection{CCA-Attention table}
        Приведем пример вывода значений таблицы для одного примера:
	\nocite{*}
        \bibliographystyle{unsrt} % Use a numbered bibliography style
        \bibliography{references.bib}
	% \printbibliography
	
\end{document}
